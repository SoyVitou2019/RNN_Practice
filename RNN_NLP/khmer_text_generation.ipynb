{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\soyvi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## import the required libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from khmernltk import word_tokenize\n",
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| 2024-01-21 15:20:17,074 | \u001b[1;32mINFO\u001b[0m | khmer-nltk | Loaded model from c:\\Users\\soyvi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\khmernltk\\word_tokenize\\sklearn_crf_ner_10000.sav |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ការសិក្សា', 'ពិតជា', 'មាន', 'សារៈសំខាន់', 'ខ្លាំង', 'ណាស់']\n"
     ]
    }
   ],
   "source": [
    "text = \"ការសិក្សាពិតជាមានសារៈសំខាន់ខ្លាំងណាស់\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load text from doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(file_path):\n",
    "    text = ''\n",
    "    with fitz.open(file_path) as pdf_document:\n",
    "        for page_num in range(pdf_document.page_count):\n",
    "            page = pdf_document[page_num]\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def read_txt(file_path):\n",
    "    text = ''\n",
    "    with open(file_path, 'r', encoding='utf-8') as txt_file:\n",
    "        text = txt_file.read()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../../Web Mining/kh-search-QNA.txt\"\n",
    "text_from_txt = read_txt(file_path)\n",
    "text_from_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create corpus by lowering the letters and splitting the text by \\n\n",
    "corpus = []\n",
    "temp_text = \"\"\n",
    "ignore_word = [\" \", \"។\", \"\\n\"]\n",
    "for ch in text_from_txt:\n",
    "    if ch != \"។\":\n",
    "        if ch not in ignore_word:\n",
    "            temp_text += ch\n",
    "    else:\n",
    "        corpus.append(temp_text)\n",
    "        temp_text = \"\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Train the tokenizer and create word encoding dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_word = []\n",
    "for line in corpus:\n",
    "    tokenize_word = tokenize_word + word_tokenize(line)\n",
    "\n",
    "# get unique word\n",
    "tokenize_word = np.array(list(set(tokenize_word)))\n",
    "\n",
    "#shuffle array of word\n",
    "np.random.shuffle(tokenize_word)\n",
    "\n",
    "# calculate vocabulary size + 1 for <oov> token\n",
    "vocab_size = len(tokenize_word) + 1\n",
    "\n",
    "print(tokenize_word)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove longest word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 20\n",
    "filtered_words = [word for word in tokenize_word if len(word) <= max_length]\n",
    "\n",
    "def remove_longest_word(max_length, tokenize_word):\n",
    "    filtered_words = [word for word in tokenize_word if len(word) <= max_length]\n",
    "    return np.array(filtered_words)\n",
    "## re assign filtered_words to tokenized_word\n",
    "tokenize_word = remove_longest_word(max_length, tokenize_word)\n",
    "tokenize_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create N-gram sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create n-gram sequences of each text sequence\n",
    "input_sequences = []\n",
    "for line in corpus:\n",
    "    tokens = word_tokenize(line)\n",
    "    tokens = remove_longest_word(max_length, tokenize_word)\n",
    "    for i in range(2, len(tokens) + 1):\n",
    "        temp_array_seq = []\n",
    "        for word in tokens[: i]:\n",
    "            temp_array_seq.append(list(tokenize_word).index(word))\n",
    "        input_sequences.append(temp_array_seq)\n",
    "\n",
    "print(input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pad sequences\n",
    "max_seq_len = max([len(i) for i in input_sequences])\n",
    "input_seq_array = np.array(pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 Extract features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating features(x) and label(y)\n",
    "x = input_seq_array[:, :-1]\n",
    "labels = input_seq_array[:, -1]\n",
    "# one-hot encode the labels to get y\n",
    "y = tf.keras.utils.to_categorical(labels, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab_size, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 64, input_length=max_seq_len - 1)\n",
    "])\n",
    "model1.weights[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 64, input_length = max_seq_len - 1),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "history = model.fit(x, y, epochs=240, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metric(history, metric):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(history=history, metric='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = \"ធ្វើដូចម្តេងទើងយើងស៊ីឯបនឆ្ងាញ់ពីសា\"\n",
    "\n",
    "seed_text = [seed_text]\n",
    "\n",
    "test_sequence = []\n",
    "\n",
    "for line in seed_text:\n",
    "    tokens = word_tokenize(line)\n",
    "    for i in range(2, len(tokens) + 1):\n",
    "        temp_array_seq = []\n",
    "        for word in tokens[: i]:\n",
    "            temp_array_seq.append(list(tokenize_word).index(word))\n",
    "        test_sequence.append(temp_array_seq)\n",
    "\n",
    "# tokenlist = tokenizer.texts_to_sequences([seed_text])\n",
    "token_pad = pad_sequences(test_sequence, maxlen=max_seq_len - 1, padding='pre')\n",
    "predictd = model.predict(token_pad, verbose=0)\n",
    "print(np.max(predictd), np.argmax(predictd, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(text, tokenize_word):\n",
    "    text_seq = []\n",
    "    for line in [text]:\n",
    "        tokens = word_tokenize(line)\n",
    "        for i in range(2, len(tokens) + 1):\n",
    "            temp_array_seq = []\n",
    "            previous_word = \"\"\n",
    "            for word in tokens[: i]:\n",
    "                if word not in ignore_word:\n",
    "                    if word in tokenize_word:\n",
    "                        temp_array_seq.append(list(tokenize_word).index(word))\n",
    "                    else:\n",
    "                        if previous_word + word not in tokenize_word:\n",
    "                            pass\n",
    "                        else:\n",
    "                            temp_array_seq.append(list(tokenize_word).index(previous_word + word))\n",
    "                previous_word = word\n",
    "            text_seq.append(temp_array_seq)\n",
    "    return text_seq[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = \"ធ្វើដូចម្តេងទើងយើងស៊ីឯបនឆ្ងាញពីសា\"\n",
    "\n",
    "\n",
    "## add number of words you want to predict\n",
    "next_words = 25\n",
    "\n",
    "## run the loop to predict and concatenate the word\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = text_to_sequence(seed_text, tokenize_word)\n",
    "    token_pad = pad_sequences([token_list], maxlen=max_seq_len - 1, padding='pre')\n",
    "    # predict the class using the trained model\n",
    "    predicted = model.predict(token_pad, verbose=0)\n",
    "    higest_prediction = np.argmax(predicted)\n",
    "    output_word = \"\"\n",
    "    for word in tokenize_word:\n",
    "        # reference the predicted class with the vocabulary\n",
    "        if higest_prediction == list(tokenize_word).index(word):\n",
    "            output_word = word\n",
    "            break\n",
    "    \n",
    "    seed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
