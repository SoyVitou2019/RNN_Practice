{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Import Libraries and APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\soyvi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\soyvi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import the required libraries and APIs\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Downloading the Tensorflow `imdb_review` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the imdb reviews datset\n",
    "data, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Segregating training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## segregate training and test set\n",
    "train_data, test_data = data['train'], data['test']\n",
    "\n",
    "# create empty list to store sentences and labels\n",
    "train_sentences = []\n",
    "test_sentences = []\n",
    "\n",
    "train_labels = []\n",
    "test_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the train data to extract sentences and labels\n",
    "for sent, label in train_data:\n",
    "    train_sentences.append(str(sent.numpy().decode('utf8')))\n",
    "    train_labels.append(label.numpy())\n",
    "\n",
    "# iterate over the test set to extract sentences and labels\n",
    "for sent, label in test_data:\n",
    "    test_sentences.append(str(sent.numpy().decode('utf8')))\n",
    "    test_labels.append(label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert lists to numpy array\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Data preparation `&` setting up the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters for the tokenizing and padding\n",
    "\n",
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 150\n",
    "trunc_type = \"post\"\n",
    "oov_tok = \"<oov>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# training sequences and labels\n",
    "train_seqs = tokenizer.texts_to_sequences(train_sentences)\n",
    "train_padded = pad_sequences(train_seqs, maxlen=max_length, truncating=trunc_type)\n",
    "\n",
    "# testing sequences and labels\n",
    "test_seqs = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_seqs, maxlen=max_length, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0   11   26   75  571\n",
      "    6  805 2354  313  106   19   12    7  629  686    6    4 2219    5\n",
      "  181  584   64 1454  110 2263    3 3951   21    2    1    3  258   41\n",
      " 4677    4  174  188   21   12 4078   11 1578 2354   86    2   20   14\n",
      " 1907    2  112  940   14 1811 1340  548    3  355  181  466    6  591\n",
      "   19   17   55 1817    5   49   14 4044   96   40  136   11  972   11\n",
      "  201   26 1046  171    5    2   20   19   11  294    2 2155    5   10\n",
      "    3  283   41  466    6  591    5   92  203    1  207   99  145 4382\n",
      "   16  230  332   11 2486  384   12   20   31   30]\n",
      "? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? i have been known to fall asleep during films but this is usually due to a combination of things including really tired being warm and comfortable on the <oov> and having just eaten a lot however on this occasion i fell asleep because the film was rubbish the plot development was constant constantly slow and boring things seemed to happen but with no explanation of what was causing them or why i admit i may have missed part of the film but i watched the majority of it and everything just seemed to happen of its own <oov> without any real concern for anything else i cant recommend this film at all\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "\n",
    "print(train_sentences[1])\n",
    "print(train_padded[1])\n",
    "print(decode_review(train_padded[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 style=\"color:orange; font-weight:bold\" > - Define the Neural Network with Embedding layer </h3>\n",
    "\n",
    "1. Use the Sequential API\n",
    "2. Add an embedding input layer of input size equal to vocabulary size.\n",
    "3. Add a flatten layer, and two dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 150, 16)           160000    \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2400)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 14406     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 174413 (681.30 KB)\n",
      "Trainable params: 174413 (681.30 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# compile the model with loss function optimizer and metrics\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03420449 -0.00760012  0.0475404  ...  0.03849551  0.00735551\n",
      "   0.04970194]\n",
      " [ 0.00293024 -0.02019421 -0.01189305 ... -0.02738918 -0.04886878\n",
      "  -0.01985217]\n",
      " [ 0.04417821  0.03829149 -0.03400198 ...  0.01464446  0.03809166\n",
      "   0.00796164]\n",
      " ...\n",
      " [-0.04529114  0.03091918  0.04217918 ...  0.04098041  0.02455306\n",
      "   0.02421195]\n",
      " [ 0.03861166  0.01928907 -0.02539095 ...  0.03343615  0.00397048\n",
      "  -0.01606653]\n",
      " [ 0.02232195  0.01020239  0.02450914 ... -0.02841574  0.0334674\n",
      "  -0.03789992]]\n"
     ]
    }
   ],
   "source": [
    "first_layer_weights = model.layers[0].get_weights()[0]\n",
    "print(first_layer_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:orange; font-weight:bold\" > - Model Training </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\soyvi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\soyvi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\soyvi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\soyvi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 11s 11ms/step - loss: 0.4716 - accuracy: 0.7544 - val_loss: 0.3577 - val_accuracy: 0.8450\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.2234 - accuracy: 0.9149 - val_loss: 0.3939 - val_accuracy: 0.8303\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.0865 - accuracy: 0.9783 - val_loss: 0.4790 - val_accuracy: 0.8234\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 11s 15ms/step - loss: 0.0233 - accuracy: 0.9966 - val_loss: 0.5630 - val_accuracy: 0.8216\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.0052 - accuracy: 0.9997 - val_loss: 0.6508 - val_accuracy: 0.8214\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6829 - val_accuracy: 0.8259\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 8.1335e-04 - accuracy: 1.0000 - val_loss: 0.7329 - val_accuracy: 0.8258\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 5s 7ms/step - loss: 4.4631e-04 - accuracy: 1.0000 - val_loss: 0.7736 - val_accuracy: 0.8261\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 2.5711e-04 - accuracy: 1.0000 - val_loss: 0.8177 - val_accuracy: 0.8257\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 5s 7ms/step - loss: 1.5429e-04 - accuracy: 1.0000 - val_loss: 0.8577 - val_accuracy: 0.8254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1a7681aecd0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "# training the model with training and vilidation set\n",
    "model.fit(\n",
    "    train_padded,\n",
    "    train_labels,\n",
    "    epochs = num_epochs,\n",
    "    validation_data = (test_padded, test_labels)\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:orange; font-weight:bold\" > - Deriving weights from the embedding layer </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n",
      "[[ 1.79723985e-02 -9.03354213e-03  2.34014709e-02 ...  1.87363438e-02\n",
      "   1.60090160e-02  9.78506505e-02]\n",
      " [ 3.90011147e-02 -2.70659942e-02 -3.55541259e-02 ... -7.54991546e-02\n",
      "   1.34516815e-02  1.17816284e-01]\n",
      " [ 4.01189476e-02 -5.44104958e-03 -5.31555936e-02 ...  6.73783338e-03\n",
      "   4.73823659e-02  1.27314672e-01]\n",
      " ...\n",
      " [-1.23997763e-01 -1.24556664e-02  1.39235288e-01 ...  1.61552966e-01\n",
      "   6.30636737e-02 -5.33563793e-02]\n",
      " [ 4.59713191e-02 -7.47568980e-02  1.39058709e-01 ...  1.72270581e-01\n",
      "   8.01814795e-02 -4.26091701e-02]\n",
      " [-7.77987912e-02 -1.46989487e-02  1.46190122e-01 ... -1.71850348e-04\n",
      "  -1.21196680e-01  6.06914498e-02]]\n"
     ]
    }
   ],
   "source": [
    "# isolating the first embedding layer\n",
    "l1 = model.layers[0]\n",
    "\n",
    "# extract learned weights\n",
    "weights = l1.get_weights()[0]\n",
    "print(weights.shape)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
